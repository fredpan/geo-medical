import os
import json
from datetime import date
from pathlib import Path
import openai
import matplotlib.pyplot as plt

openai.api_key = os.getenv("OPENAI_API_KEY")

with open("scripts/keywords.json", "r", encoding="utf-8") as f:
    topics = json.load(f)

today = date.today().isoformat()
output_base = Path("output")

def generate_chinese_article(title_zh):
    prompt = f"""
è¯·æ’°å†™ä¸€ç¯‡åŒ»å­¦ç§‘æ™®æ–‡ç« ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œä¸»é¢˜æ˜¯ã€Œ{title_zh}ã€ã€‚

å†…å®¹åº”åŒ…æ‹¬ï¼š
1. æ¦‚å¿µè§£é‡Š
2. å¸¸è§ç—…å› 
3. ä¸´åºŠè¡¨ç°
4. æ²»ç–—æ–¹å¼
5. é¢„é˜²å»ºè®®

è¯·ç¡®ä¿å†…å®¹ç®€æ˜é€šä¿—ï¼Œé€‚åˆå¤§ä¼—é˜…è¯»ã€‚
"""
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    return response.choices[0].message["content"].strip()

def audit_with_gpt(text, topic):
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦å†…å®¹å®¡æ ¸å‘˜ã€‚è¯·å¯¹ä»¥ä¸‹æ–‡ç« è¿›è¡Œæ‰“åˆ†å¹¶æŒ‡å‡ºä¸è¶³ï¼š

æ–‡ç« ä¸»é¢˜ï¼š{topic}
å†…å®¹å¦‚ä¸‹ï¼š
{text}

è¯·è¾“å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼š
{{
  "score": æ•´ä½“è¯„åˆ†ï¼ˆ0-100ï¼‰,
  "issues": ["é—®é¢˜1", "é—®é¢˜2", ...],
  "length": å­—æ•°,
  "lang": "zh"
}}
"""
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    content = response.choices[0].message["content"]
    json_start = content.find("{")
    json_data = json.loads(content[json_start:])
    return json_data

def extract_mermaid_from_article(article_text):
    prompt = (
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ç»“æ„ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚\\n\\n"
    "è¯·ä»ä¸‹åˆ—ä¸­æ–‡åŒ»å­¦æ–‡ç« ä¸­æå–å‡ºç–¾ç—…çš„ç»“æ„æµç¨‹ï¼ˆç—…å› ã€ç—‡çŠ¶ã€æ²»ç–—ï¼‰å¹¶è¾“å‡º Mermaid æ ¼å¼æµç¨‹å›¾ã€‚\\n\\n"
    "æ–‡ç« å¦‚ä¸‹ï¼š\\n\"\"\"\\n{article}\\n\"\"\"\\n\\n"
    "åªè¾“å‡º mermaid ä»£ç æ®µï¼ˆä¸è¦è‡ªç„¶è¯­è¨€ï¼‰ï¼Œä½¿ç”¨ graph TDã€‚"
).format(article=article_text)
    res = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    content = res.choices[0].message["content"]
    return content.strip()

def generate_cover_image_prompt(title_zh):
    return f"è¯·ç”Ÿæˆä¸€å¼ ç¬¦åˆåŒ»å­¦é£æ ¼çš„å°é¢æ’å›¾ï¼Œä¸»é¢˜æ˜¯ï¼š{title_zh}ï¼Œè¦æ±‚ä¸ºç®€çº¦é£æ ¼å›¾è§£ï¼Œä¾‹å¦‚ç—…å˜æœºåˆ¶ã€ç»†èƒç»“æ„ã€æ²»ç–—è·¯å¾„ç­‰ã€‚"

def generate_medical_chart(slug, topic):
    # ç¤ºä¾‹æ•°æ®ï¼šå¯æ ¹æ® GPT è¾“å‡ºçš„ JSON åŠ¨æ€æ›¿æ¢
    labels = ['æ‰‹æœ¯', 'åŒ–ç–—', 'æ”¾ç–—', 'é¶å‘']
    sizes = [30, 25, 20, 25]
    colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']

    fig, ax = plt.subplots()
    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', startangle=90)
    ax.axis('equal')
    chart_path = output_base / "charts" / f"{slug}-plot.svg"
    fig.savefig(chart_path, format='svg')
    plt.close()

def recommend_image_sources(title_zh):
    links = [
        f"https://commons.wikimedia.org/wiki/Special:Search?search={title_zh}",
        f"https://www.ncbi.nlm.nih.gov/pmc/?term={title_zh}",
        f"https://openi.nlm.nih.gov/search?it=xg&query={title_zh}",
        f"https://www.gettyimages.com/photos/{title_zh.replace(' ', '-')}"
    ]
    return links

for topic in topics:
    slug = topic["slug"]
    title_zh = topic["zh"]
    print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆä¸­æ–‡åŒ»å­¦æ–‡ç« : {title_zh}")
    content = generate_chinese_article(title_zh)

    print(f"âœ… æ­£åœ¨å®¡æŸ¥å†…å®¹...")
    audit = audit_with_gpt(content, title_zh)

    for sub in ["geo", "readable", "wx"]:
        out_path = output_base / sub / f"{slug}.html"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(content, encoding="utf-8")

    audit_path = output_base / "audit" / f"{slug}-score.json"
    audit_path.parent.mkdir(parents=True, exist_ok=True)
    audit_path.write_text(json.dumps(audit, ensure_ascii=False, indent=2), encoding="utf-8")

    (output_base / "charts").mkdir(exist_ok=True)
    (output_base / "pdf").mkdir(exist_ok=True)
    (output_base / "charts" / f"{slug}.svg").write_text(f"<svg><text>{title_zh} å›¾è¡¨ç¤ºæ„</text></svg>", encoding="utf-8")
    (output_base / "pdf" / f"{slug}.pdf").write_text(f"{title_zh} PDF ç¤ºä¾‹", encoding="utf-8")

    print(f"ğŸ“ˆ æ­£åœ¨æå–ç»“æ„å›¾ï¼š{title_zh}")
    mermaid_code = extract_mermaid_from_article(content)
    mermaid_path = output_base / "charts" / f"{slug}.mmd"
    mermaid_path.write_text(mermaid_code, encoding="utf-8")

    print(f"ğŸ¨ å‡†å¤‡å°é¢å›¾æç¤ºè¯­ï¼š{title_zh}")
    dalle_prompt = generate_cover_image_prompt(title_zh)
    dalle_path = output_base / "charts" / f"{slug}-cover-prompt.txt"
    dalle_path.write_text(dalle_prompt, encoding="utf-8")

    print(f"ğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼š{title_zh}")
    generate_medical_chart(slug, title_zh)

    print(f"ğŸ–¼ï¸ æ¨èå¼€æ”¾å›¾åº“é“¾æ¥ï¼š{title_zh}")
    refs = recommend_image_sources(title_zh)
    ref_path = output_base / "charts" / f"{slug}-img-sources.json"
    ref_path.write_text(json.dumps(refs, ensure_ascii=False, indent=2), encoding="utf-8")
